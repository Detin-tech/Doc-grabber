ğŸ•·ï¸ Doc-grabber
Simple CLI tool for scraping and offline-saving documentation from sites that don't offer downloads.

If a site buries its docs behind JS bloat, tab hell, or lacks an export button â€” this tool rips it, cleans it, rewires links, and gives you a sane local copy. Think: minimal effort, maximum portability.

âš™ï¸ Features
Scrapes full documentation trees (static or semi-dynamic)

Rewrites internal links for offline nav

Strips trash (headers, footers, trackers)

Converts to Obsidian-style link formats (optional)

Flexible output formats: choose .md, .html, or .md + .rtf

Outputs clean local folder you can archive or browse

âš ï¸ Some features mentioned above are not yet fully implemented. They're on the roadmap â€” and yeah, that's part of why this is free for now.

ğŸ’¸ Pricing
Free right now while it's in development
Once it's stable and polished: $20/year
Grab it early if you're cool with mostly-functional jank

ğŸš§ Status
Work in progress. It works, but donâ€™t expect it to hold your hand.

Youâ€™ll want to:

Run it in a virtualenv or Docker if you're paranoid

Test it on sites you care about

File bugs or PR fixes if it explodes in edge cases

ğŸ”— Related Projects
If you're just looking for ready-to-use offline docs, check out:
**[Random-Docs](https://github.com/Detin-tech/Random-Docs)**

---

Built by [Ben](https://github.com/Detin-tech) â€” because websites shouldnâ€™t vanish and take their documentation with them.
